{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f766839d-5da7-4455-b962-5860bab507ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "86d70344-253c-4559-b07b-898728345a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\10734646\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\10734646\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\10734646\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [06/Oct/2025 02:43:47] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [06/Oct/2025 02:43:47] \"GET /static/css/styles.css HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [06/Oct/2025 02:43:59] \"POST /predict HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [06/Oct/2025 02:43:59] \"GET /static/css/styles.css HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [06/Oct/2025 02:44:00] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [06/Oct/2025 02:44:00] \"GET /static/css/styles.css HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [06/Oct/2025 02:44:08] \"POST /predict HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [06/Oct/2025 02:44:08] \"GET /static/css/styles.css HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [06/Oct/2025 02:44:09] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [06/Oct/2025 02:44:09] \"GET /static/css/styles.css HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [06/Oct/2025 02:45:55] \"POST /predict HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [06/Oct/2025 02:45:55] \"GET /static/css/styles.css HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [06/Oct/2025 02:45:56] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [06/Oct/2025 02:45:56] \"GET /static/css/styles.css HTTP/1.1\" 404 -\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, render_template\n",
    "import pickle\n",
    "import numpy as np\n",
    "import gensim\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Load models\n",
    "classifier = pickle.load(open('model.pkl', 'rb'))\n",
    "w2v_model = gensim.models.Word2Vec.load('word2vec.model')\n",
    "\n",
    "# Preprocessing function\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_and_vectorize(text):\n",
    "    text = text.lower()\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word.isalpha() and word not in stop_words]\n",
    "    word_vectors = [w2v_model.wv[word] for word in tokens if word in w2v_model.wv]\n",
    "    if word_vectors:\n",
    "        return np.mean(word_vectors, axis=0).reshape(1, -1)\n",
    "    else:\n",
    "        return np.zeros((1, w2v_model.vector_size))  # Handle OOV cases\n",
    "\n",
    "@app.route('/')\n",
    "def home():\n",
    "    return render_template('index.html')\n",
    "\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    input_text = request.form['message']\n",
    "    vector = preprocess_and_vectorize(input_text)\n",
    "    prediction = classifier.predict(vector)[0]\n",
    "    # Pass the prediction to result.html\n",
    "    return render_template('result.html', prediction=int(prediction))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(debug=True,use_reloader=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "419ea119-6f43-4697-98cd-b65e199e6120",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook app.ipynb to script\n",
      "[NbConvertApp] Writing 707 bytes to app.py\n"
     ]
    }
   ],
   "source": [
    "# !jupyter nbconvert --to script app.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc80e85-6dcb-418d-8bb0-4bdb76508147",
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nbconvert --to script model.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5f2e52f1-49f5-4442-9851-c3e6a4dc1e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Not scanning for jupyter notebooks.\n",
      "WARNING: Import named \"numpy\" not found locally. Trying to resolve it at the PyPI server.\n",
      "WARNING: Import named \"numpy\" was resolved to \"numpy:2.3.3\" package (https://pypi.org/project/numpy/).\n",
      "Please, verify manually the final list of requirements.txt to avoid possible dependency confusions.\n",
      "WARNING: Import named \"pandas\" not found locally. Trying to resolve it at the PyPI server.\n",
      "WARNING: Import named \"pandas\" was resolved to \"pandas:2.3.3\" package (https://pypi.org/project/pandas/).\n",
      "Please, verify manually the final list of requirements.txt to avoid possible dependency confusions.\n",
      "WARNING: Import named \"scikit_learn\" not found locally. Trying to resolve it at the PyPI server.\n",
      "WARNING: Import named \"scikit_learn\" was resolved to \"scikit-learn:1.7.2\" package (https://pypi.org/project/scikit-learn/).\n",
      "Please, verify manually the final list of requirements.txt to avoid possible dependency confusions.\n",
      "INFO: Successfully saved requirements file in .\\requirements.txt\n"
     ]
    }
   ],
   "source": [
    " # !pipreqs ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8f1d5a1d-6044-4a03-b1f6-b98883a42a9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [06/Oct/2025 02:21:50] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [06/Oct/2025 02:21:50] \"GET /static/css/styles.css HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [06/Oct/2025 02:22:01] \"POST /predict HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [06/Oct/2025 02:22:01] \"GET /static/css/styles.css HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [06/Oct/2025 02:22:03] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [06/Oct/2025 02:22:03] \"GET /static/css/styles.css HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [06/Oct/2025 02:22:11] \"POST /predict HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [06/Oct/2025 02:22:11] \"GET /static/css/styles.css HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [06/Oct/2025 02:22:13] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [06/Oct/2025 02:22:13] \"GET /static/css/styles.css HTTP/1.1\" 404 -\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask,render_template,url_for,request\n",
    "import pandas as pd \n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import joblib\n",
    "import pickle\n",
    "\n",
    "# load the model from disk\n",
    "filename = 'nlp_model.pkl'\n",
    "clf = pickle.load(open(filename, 'rb'))\n",
    "cv=pickle.load(open('tranform.pkl','rb'))\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/')\n",
    "def home():\n",
    "\treturn render_template('index.html')\n",
    "\n",
    "@app.route('/predict',methods=['POST'])\n",
    "def predict():\n",
    "\n",
    "\n",
    "\tif request.method == 'POST':\n",
    "\t\tmessage = request.form['message']\n",
    "\t\tdata = [message]\n",
    "\t\tvect = cv.transform(data).toarray()\n",
    "\t\tmy_prediction = clf.predict(vect)\n",
    "\treturn render_template('result.html',prediction = my_prediction)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\tapp.run(debug=True,use_reloader=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547a710b-a10a-470a-8240-74b67bca91b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5e8e451b-4941-4578-af5a-2f518220bdfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\10734646\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\10734646\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\10734646\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [06/Oct/2025 02:59:53] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [06/Oct/2025 02:59:53] \"GET /static/css/styles.css HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [06/Oct/2025 03:00:01] \"POST /predict HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [06/Oct/2025 03:00:01] \"GET /static/css/styles.css HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [06/Oct/2025 03:00:03] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [06/Oct/2025 03:00:03] \"GET /static/css/styles.css HTTP/1.1\" 404 -\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, render_template\n",
    "import pickle\n",
    "import numpy as np\n",
    "import gensim\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Load models\n",
    "classifier = pickle.load(open('word2vec_spam_model.pkl', 'rb'))\n",
    "w2v_model = gensim.models.Word2Vec.load('word2vec.model')\n",
    "\n",
    "# Preprocessing function\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_and_vectorize(text):\n",
    "    text = text.lower()\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word.isalpha() and word not in stop_words]\n",
    "    word_vectors = [w2v_model.wv[word] for word in tokens if word in w2v_model.wv]\n",
    "    if word_vectors:\n",
    "        return np.mean(word_vectors, axis=0).reshape(1, -1)\n",
    "    else:\n",
    "        return np.zeros((1, w2v_model.vector_size))  # Handle OOV cases\n",
    "\n",
    "@app.route('/')\n",
    "def home():\n",
    "    return render_template('index.html')\n",
    "\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    input_text = request.form['message']\n",
    "    vector = preprocess_and_vectorize(input_text)\n",
    "    prediction = classifier.predict(vector)[0]\n",
    "    # Pass the prediction to result.html\n",
    "    return render_template('result.html', prediction=int(prediction))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(debug=True,use_reloader=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ec420c-ca70-4766-9160-f8bed70a0362",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
